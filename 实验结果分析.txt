1. 实验设置：只改进DDQ agent的经验池采样。（两倍采样，根据DDQ agent在样本上的损失由大到小取一半样本）
baseline: DDQ
配置：K=10，epoch=500，20个实验结果的平均精度
实验结果：在前100轮可以看见效果提升，在后几百轮效果反而下降了

原因分析：
    1.在后期，损失大的经验，可能是大部分来自于world model，而world model拟合real user有比较大的差异。
    2.replay pool更新比较慢，其中旧的经验占了很大部分，而旧经验损失大